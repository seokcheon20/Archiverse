---
title: Academic Integrity and AI
tags:
  - ai
  - misc
  - seedling
date: 2024-09-14
lastmod: 2024-10-23
draft: false
---
Recent studies reveal that the use of AI is becoming increasingly common in academic writings. On [Google Scholar](https://misinforeview.hks.harvard.edu/article/gpt-fabricated-scientific-papers-on-google-scholar-key-features-spread-and-implications-for-preempting-evidence-manipulation/), and on [arXiv](https://arxiv.org/abs/2403.13812); but most shockingly, on platforms like Elsevier's [Science Direct](http://web.archive.org/web/20240315011933/https://www.sciencedirect.com/science/article/abs/pii/S2468023024002402) (check the Introduction). Elsevier supposedly prides itself on its comprehensive review process, which ensures that its publications are of the highest quality. More generally, the academic *profession* insists that it possesses what I call [[Atomic/integrity|integrity]]: rigor, attention to detail, and authority or credibility. But AI is casting light on a greater issue: **does it**?
## Competing framings
I think there are two ways of framing the emergence of the problem.
### 1: Statistical (not dataset) bias and sample size
The first framing is simple proportionality. Journal submission numbers have increased rapidly in the past few years: [atherosclerosis](https://www.atherosclerosis-journal.com/article/S0021-9150(13)00456-5/abstract)
![[Attachments/papers.png|graph showing almost an exponential trend in paper submission from 1970 (about 5 million) to 2013 (over 40 million).]]

This trend has remained consistent in the past decade, I just couldn't find as nice a graphic. [science.org](https://science.org/content/page/journal-metrics) shows nearly 40 thousand submissions over its corpus in 2023. Thus, it's natural that more low-quality papers would "slip through the cracks" or similar.

I'm not interested in doing the statistical analysis (especially because it would probably require creating a quantitative analysis metric for integrity, which I super don't want to spend time on), but this is just one hypothesis.

I am, however, aware of the argument that there's nothing inherently bad about a rise in submissions in and of itself. I agree with that argument in that such a claim would be unsubstantiated. For example, the above Atherosclerosis article uses the graph as evidence of what it calls "filter failure." I don't think that's the case. Instead, it may be indicative of a different systematic problem being *solved*: barriers to access are coming down as access to education improves. But AI removes this cause for celebration, because by degrading the integrity of the journals by which we measure the progress, it lowers the importance of that achievement.
### 2: We've Always Done It This Way
Second, which I find more persuasive (but shocking), is the question: has it just always been like this? 

There's a possibility that the thought of academic integrity is just a facade meant to preserve the aforementioned barriers to access. I do understand how it can be seen as hegemonic in nature. Detail requires (paid) time, intellectual rigor requires education, credibility requires access to information, authority requires experience...

If that's the case, I'd definitely advocate for a shift in academic norms. Think of it like moving the Venn diagram circles around a bit in a way that we can accomplish all of:
- Preserving the thought of journals as having the new conception of integrity;
- Continuing to classify AI works as lacking integrity; and
- Dismantling the bad-faith barriers to access presented by the old normative definition.
## Further Reading
In my view, a critical component of academic or purportedly informative works is the establishment of authority/credibility in a way that's verifiable by other people. I have an incomplete [[Essays/plagiarism|essay on plagiarism]] where I explore this facet.

I subscribe to a style of writing called "academ*ish*" voice on this site, documented by [Ink and Switch](https://inkandswitch.notion.site/Academish-Voice-0d8126b3be5545d2a21705ceedb5dd45). Pointing out all the ways that even this less-serious style is fundamentally incompatible with AI-generated text is left as an exercise for the reader.